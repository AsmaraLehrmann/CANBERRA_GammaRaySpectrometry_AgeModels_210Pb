{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d518608",
   "metadata": {},
   "source": [
    "# Welcome to the 210Pb age model script v2.2!\n",
    "\n",
    "### <div style=\"text-align: right\"> Last modified by A.A. Lehrmann 4 September 2025 </div>\n",
    "\n",
    "\n",
    "### The script below will extract radioisotope data from Canberra PDFs, run the age model (from the Wellner Lab Group excel model (Appleby, 2001; Boldt et al., 2013), and plot the age model\n",
    "\n",
    "### Important instructions before you begin:\n",
    "\n",
    "    1. NEVER edit raw data. Do not delete Canberra PDFs. Do not remove sediment weights from original lab notebook excel sheet.\n",
    "\n",
    "    2. Make an /CORE_AgeModelOutput/ folder to put all of your script's outputs\n",
    "\n",
    "    3. When copying folder paths, make sure to remove quotation marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a513b53",
   "metadata": {},
   "source": [
    "## First, extract radioisotope data from Canberra PDFs\n",
    "Run cell (press triangle that says run) and follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674ea42-dfe2-402d-b629-7919e8eb4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: prefer highest version (_v2, _v3, …) for BOTH Canberra and PtSrc files ---\n",
    "import os, re, numpy as np, pandas as pd\n",
    "\n",
    "# Capture depth AND optional version (e.g., 21-24, 21-24cm, 21-24_v2, 21-24cm_v3, etc.)\n",
    "DEPTH_RE = re.compile(r'(\\d{1,3}-\\d{1,3})(?:cm)?(?:_v(\\d+))?(?=\\.|_|$)', re.IGNORECASE)\n",
    "\n",
    "def _depth_and_version(s: str):\n",
    "    base = os.path.basename(str(s))\n",
    "    m = DEPTH_RE.search(base)\n",
    "    if not m:\n",
    "        return None, 0\n",
    "    depth = m.group(1)                 # standardized like \"21-24\"\n",
    "    version = int(m.group(2)) if m.group(2) else 0\n",
    "    return depth, version\n",
    "\n",
    "def reshape_canberra_with_ptsrc_mixed(df_in: pd.DataFrame,\n",
    "                                      write_to: str | None = None,\n",
    "                                      ptsrc_cols: tuple[str,str] = (\"Pb-210\", \"Pb-210 error\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Works on a single mixed table (Canberra + PtSrc rows).\n",
    "    - For any duplicate depths on either side, keeps the **highest _vN**.\n",
    "    - Adds H/I/J = ['ptsrc_pb210','ptsrc_pb210 error','file ptsrc'] right after 'Pb-214 error'.\n",
    "    - Preserves your original header names (no renaming of 'error' columns).\n",
    "    \"\"\"\n",
    "    if \"File\" not in df_in.columns:\n",
    "        raise KeyError(\"Input dataframe must contain a 'File' column.\")\n",
    "\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Split rows by prefix\n",
    "    is_ptsrc = df[\"File\"].astype(str).str.startswith(\"PtSrc_\")\n",
    "    can_df = df.loc[~is_ptsrc].copy()\n",
    "    pt_df  = df.loc[ is_ptsrc].copy()\n",
    "\n",
    "    # Extract (depth, version) for both sides\n",
    "    can_df[[\"__depth__\",\"__ver__\"]] = can_df[\"File\"].apply(lambda s: pd.Series(_depth_and_version(s)))\n",
    "    pt_df[[\"__depth__\",\"__ver__\"]]  = pt_df[\"File\"].apply(lambda s: pd.Series(_depth_and_version(s)))\n",
    "\n",
    "    # **Prefer highest version for Canberra** when duplicates share the same depth\n",
    "    can_df = can_df.sort_values(\"__ver__\").drop_duplicates(subset=\"__depth__\", keep=\"last\")\n",
    "\n",
    "    # Pick value/error columns in PtSrc rows\n",
    "    val_col, err_col = ptsrc_cols\n",
    "    if val_col not in pt_df.columns or err_col not in pt_df.columns:\n",
    "        # Fallback: first two numeric columns\n",
    "        num_cols = [c for c in pt_df.columns if c != \"File\" and np.issubdtype(pt_df[c].dtype, np.number)]\n",
    "        if len(num_cols) < 2:\n",
    "            for c in pt_df.columns:\n",
    "                if c != \"File\":\n",
    "                    pt_df[c] = pd.to_numeric(pt_df[c], errors=\"coerce\")\n",
    "            num_cols = [c for c in pt_df.columns if c != \"File\" and np.issubdtype(pt_df[c].dtype, np.number)]\n",
    "        val_col, err_col = num_cols[:2]\n",
    "\n",
    "    # Reduce PtSrc to needed columns and **prefer highest version per depth**\n",
    "    q = pt_df.loc[:, [\"__depth__\", \"__ver__\", \"File\", val_col, err_col]].copy()\n",
    "    q.rename(columns={\n",
    "        \"File\": \"file ptsrc\",\n",
    "        val_col: \"ptsrc_pb210\",\n",
    "        err_col: \"ptsrc_pb210 error\"\n",
    "    }, inplace=True)\n",
    "    q[\"file ptsrc\"] = q[\"file ptsrc\"].map(lambda x: os.path.basename(str(x)))\n",
    "    q = q.sort_values(\"__ver__\").drop_duplicates(subset=\"__depth__\", keep=\"last\")\n",
    "\n",
    "    # Merge and clean\n",
    "    out = can_df.merge(q.drop(columns=\"__ver__\"), on=\"__depth__\", how=\"left\")\n",
    "    out.drop(columns=[\"__depth__\",\"__ver__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Desired column order (keep original header names)\n",
    "    base = [\"File\",\"Pb-210\",\"Pb-210 error\",\"Bi-214\",\"Bi-214 error\",\"Pb-214\",\"Pb-214 error\"]\n",
    "    hij  = [\"ptsrc_pb210\",\"ptsrc_pb210 error\",\"file ptsrc\"]\n",
    "    others = [col for col in out.columns if col not in base + hij]\n",
    "    out = out[[c for c in base if c in out.columns] + hij + others]\n",
    "\n",
    "    if write_to:\n",
    "        out.to_csv(write_to, index=False)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda88dac-3fcf-4c27-b2ab-c9c3e8a60701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add age model information\n",
    "SCRIPT_VERSION = \"v2.2\"\n",
    "OPERATOR_NAME = input(\"Operator name: \").strip()\n",
    "_default = datetime.today().strftime(\"%Y%m%d\")\n",
    "_run = input(f\"Run date [YYYYMMDD] (Enter for { _default }): \").strip() or _default\n",
    "RUN_DATE = _run\n",
    "CORE_NAME_RAW = input(\"Core name (e.g., MB1901): \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6b0e9-a219-4ed8-996e-55baf5da2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#session metadata\n",
    "CORE_NAME = re.sub(r\"[^A-Za-z0-9]+\", \"\", CORE_NAME_RAW)\n",
    "SUFFIX = f\"_{CORE_NAME}_{RUN_DATE}\"\n",
    "META = {\"operator\": OPERATOR_NAME, \"date\": RUN_DATE, \"core\": CORE_NAME, \"version\": SCRIPT_VERSION, \"suffix\": SUFFIX}\n",
    "\n",
    "def compose_output_name(basename: str, ext: str, directory: Path | str = Path.cwd()) -> Path:\n",
    "    \"\"\"\n",
    "    Returns a full Path like <directory>/<basename>_<CoreName_YYYYMMDD><ext>\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    base = f\"{basename}{META['suffix']}\"\n",
    "    if not ext.startswith(\".\"):\n",
    "        ext = \".\" + ext\n",
    "    return directory / f\"{base}{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23304f95-a0f8-4f28-8c7f-697050a5dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write readme files\n",
    "def write_readme(for_path: Path):\n",
    "    \"\"\"\n",
    "    Creates: <basename>_README_<YYYYMMDD>.txt next to the file.\n",
    "    Prompts for optional notes.\n",
    "    \"\"\"\n",
    "    notes = input(f\"Notes for {for_path.name} (optional): \").strip()\n",
    "    base_no_ext = for_path.with_suffix(\"\").name\n",
    "    readme_name = f\"{base_no_ext}_README_{META['date']}.txt\"\n",
    "    readme_path = for_path.parent / readme_name\n",
    "    content = [\n",
    "        f\"Output file: {for_path.name}\",\n",
    "        f\"Operator: {META['operator']}\",\n",
    "        f\"Run date (YYYYMMDD): {META['date']}\",\n",
    "        f\"Script version: {META['version']}\",\n",
    "        f\"Core name: {META['core']}\",\n",
    "        f\"Notes: {notes if notes else '(none)'}\",\n",
    "        \"\"\n",
    "    ]\n",
    "    readme_path.write_text(\"\\n\".join(content), encoding=\"utf-8\")\n",
    "    print(f\"README created -> {readme_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937424bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process pt_src files\n",
    "def process_ptsrc_pdf(file_path, filename):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        if len(reader.pages) < 3:\n",
    "            print(f\"PDF file '{filename}' has less than 3 pages. Skipping.\")\n",
    "            return None\n",
    "        page = reader.pages[2]\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Pb-210' in line:\n",
    "                ptsrc_pb210, PtSrc_Pb210error = line.split()[-2:]\n",
    "                return {\n",
    "                    'File': filename,\n",
    "                    'Pb-210': float(ptsrc_pb210),\n",
    "                    'Pb-210 error': float(PtSrc_Pb210error)\n",
    "                }\n",
    "        print(f\"Pb-210 not found in '{filename}'. Skipping.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF file '{filename}': {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0995561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process regular files\n",
    "def process_regular_pdf(file_path, filename):\n",
    "    pb210 = pb210error = Bi214 = Bi214error = Pb214 = Pb214error = None\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        if len(reader.pages) < 3:\n",
    "            print(f\"PDF file '{filename}' has less than 3 pages. Skipping.\")\n",
    "            return None\n",
    "        page = reader.pages[2]\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Pb-210' in line:\n",
    "                pb210, pb210error = line.split()[-2:]\n",
    "            elif 'Bi-214' in line:\n",
    "                Bi214, Bi214error = line.split()[-2:]\n",
    "            elif 'Pb-214' in line:\n",
    "                Pb214, Pb214error = line.split()[-2:]\n",
    "        if pb210 is None or pb210error is None:\n",
    "            print(f\"Pb-210 not found in '{filename}'. Skipping.\")\n",
    "            return None\n",
    "        if Bi214 is None or Bi214error is None:\n",
    "            print(f\"Bi-214 not found in '{filename}'. Skipping.\")\n",
    "            return None\n",
    "        if Pb214 is None or Pb214error is None:\n",
    "            print(f\"Pb-214 not found in '{filename}'. Skipping.\")\n",
    "            return None\n",
    "        return {\n",
    "            'File': filename,\n",
    "            'Pb-210': float(pb210),\n",
    "            'Pb-210 error': float(pb210error),\n",
    "            'Bi-214': float(Bi214),\n",
    "            'Bi-214 error': float(Bi214error),\n",
    "            'Pb-214': float(Pb214),\n",
    "            'Pb-214 error': float(Pb214error)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF file '{filename}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cef998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define PDF processing\n",
    "def process_pdf_file(file_path, filename):\n",
    "    if filename.startswith(\"PtSrc_\"):\n",
    "        return process_ptsrc_pdf(file_path, filename)\n",
    "    else:\n",
    "        return process_regular_pdf(file_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43489c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract PDF data from folder\n",
    "def extract_pdf_data(folder_path):\n",
    "    combined_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check for PDF extension (case-insensitive)\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = process_pdf_file(file_path, filename)\n",
    "            if data is not None:\n",
    "                combined_data.append(data)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort and process PDFs\n",
    "def sort_pdf_data(combined_data, parse_numbers=False):\n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    def extract_numeric_suffix(file_name):\n",
    "        try:\n",
    "            parts = file_name.split('_')[-1].split('.')[0]\n",
    "            return int(parts)\n",
    "        except ValueError:\n",
    "            return float('nan')\n",
    "    \n",
    "    combined_df['File_order'] = combined_df['File'].apply(extract_numeric_suffix)\n",
    "    combined_df = combined_df.sort_values(by='File_order').drop(columns=['File_order'])\n",
    "    \n",
    "    if parse_numbers:\n",
    "        for col in ['Pb-210', 'Bi-214', 'Pb-214']:\n",
    "            if col in combined_df.columns:\n",
    "                combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "def extract_pdf_values(folder_path, output_csv_path, parse_numbers=False):\n",
    "    combined_data = extract_pdf_data(folder_path)\n",
    "    combined_df = sort_pdf_data(combined_data, parse_numbers)\n",
    "\n",
    "    # Use the helper to match by depth and insert H/I/J (ptsrc, ptsrc_error, file ptsrc)\n",
    "    final_df = reshape_canberra_with_ptsrc_mixed(combined_df, write_to=output_csv_path)\n",
    "\n",
    "    csv_path = Path(output_csv_path)\n",
    "    print(f\"CSV saved -> {csv_path}\")\n",
    "    write_readme(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a50848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "folder_path = input(\"Enter the folder path of Canberra PDFs: \").strip()\n",
    "\n",
    "# Auto-generate CSV output name based on metadata\n",
    "output_csv_path = compose_output_name(\"CanberraData\", \".csv\", Path(folder_path))\n",
    "print(f\"CSV will be saved as -> {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute functions\n",
    "extract_pdf_values(folder_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7568417",
   "metadata": {},
   "source": [
    "### Make note of which samples are missing data! This will be important when we plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f82fd",
   "metadata": {},
   "source": [
    "# Create a new .csv file from lab notebook for the sample weight data\n",
    "\n",
    "Sample weights should have the following headings: \n",
    "\n",
    "### | Core    | Top of interval (cm)   | Center point of interval    |Base of interval (cm)  | sediment weight (g)    | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b5ac2",
   "metadata": {},
   "source": [
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776436fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Prompt user for file paths\n",
    "csv1_path = input(\"Enter the path to the sample weight CSV file (e.g., /path/weights.csv): \").strip()\n",
    "csv2_path = input(\"Enter the path to the Canberra data CSV file (e.g., /path/canberra.csv): \").strip()\n",
    "\n",
    "# Use Canberra CSV's folder as output directory\n",
    "out_dir = Path(csv2_path).parent\n",
    "\n",
    "# Auto-generate Age Model output name\n",
    "output_file_name = compose_output_name(\"AgeModel\", \".csv\", out_dir)\n",
    "print(f\"Output file will be saved as -> {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e60839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and merge data\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(csv1_path)\n",
    "csv2 = pd.read_csv(csv2_path)\n",
    "\n",
    "# Extract 'Center point of interval' from csv2 based on the median of the last digits in 'File'\n",
    "csv2['Center point of interval'] = csv2['File'].apply(\n",
    "    lambda x: np.median([int(num) for num in re.findall(r'\\d+', x.split('_')[-1])])\n",
    ")\n",
    "\n",
    "# Merge CSV files on 'Center point of interval'\n",
    "data = pd.merge(csv1, csv2, on='Center point of interval', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for the year of core\n",
    "year_of_core = int(input(\"Enter the year of core (e.g., 2023): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4c96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate activity and correction factors\n",
    "data['Pb-210 activity Uncertainty (Bq-g)'] = data['Pb-210 error']/data['sediment weight (g)']\n",
    "data['Pb-210 activity (Bq/g)'] = data['Pb-210'] / data['sediment weight (g)']\n",
    "data['Pb-210 correction factor'] = data['ptsrc_pb210'] / 151031.56\n",
    "data['Self absorb. Corrected Pb-210 activity (Bq/g)'] = (\n",
    "    data['Pb-210 activity (Bq/g)'] / data['Pb-210 correction factor']\n",
    ")\n",
    "data['Bi-214 activity (Bq/g)'] = data['Bi-214'] / data['sediment weight (g)']\n",
    "data['Pb-214 activity (Bq/g)'] = data['Pb-214'] / data['sediment weight (g)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf63be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averaged supported activity of Bi-214 and Pb-214 (Bq/g)\n",
    "data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)'] = (\n",
    "    data['Bi-214 activity (Bq/g)'] + data['Pb-214 activity (Bq/g)']\n",
    ") / 2\n",
    "\n",
    "# Calculate background activity uncertainty (Bq/g)\n",
    "data['Background activity uncertainty (Bq/g)'] = (\n",
    "    (data['Bi-214 error'] + data['Pb-214 error']) / 2\n",
    ") / data['sediment weight (g)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01701f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Excess Pb-210 (Bq/g)\n",
    "data['Excess Pb-210 (Bq/g)'] = (\n",
    "    data['Self absorb. Corrected Pb-210 activity (Bq/g)'] -\n",
    "    data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)']\n",
    ")\n",
    "\n",
    "# Determine surface activity (first interval value)\n",
    "data['Surface activity'] = data['Excess Pb-210 (Bq/g)'].iloc[0]\n",
    "\n",
    "# Calculate Age bp using the natural logarithm\n",
    "data['Age bp'] = (1 / 0.03114) * np.log(data['Surface activity'] / data['Excess Pb-210 (Bq/g)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ee0a3-542e-41d4-8c9f-bd3bc03734da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, basename: str, directory=Path.cwd(), index=False):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame as CSV with _CoreNameYYYYMMDD suffix and README.\n",
    "    \"\"\"\n",
    "    out_path = compose_output_name(basename, \".csv\", directory)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=index)\n",
    "\n",
    "    print(f\"CSV saved -> {out_path}\")\n",
    "    write_readme(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1351d3e-beb4-4ea2-9e86-2a65bffe3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def _resolve_csv_target(output_csv_path, default_basename=\"pdf_extraction\"):\n",
    "    \"\"\"\n",
    "    Accepts either a directory or a file path. \n",
    "    - If a directory or a path without extension is provided, creates it if needed and\n",
    "      returns <directory>/<default_basename>_CoreNameYYYYMMDD.csv using compose_output_name().\n",
    "    - If a .csv file path is provided, returns it as-is (and ensures parent exists).\n",
    "    \"\"\"\n",
    "    p = Path(output_csv_path)\n",
    "\n",
    "    # Explicit .csv file path\n",
    "    if p.suffix.lower() == \".csv\":\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        return p\n",
    "\n",
    "    # Treat as a directory (existing or to be created)\n",
    "    if p.exists() and p.is_dir():\n",
    "        return compose_output_name(default_basename, \".csv\", p)\n",
    "\n",
    "    if p.suffix == \"\":\n",
    "        # looks like a folder path without extension; make it a directory\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return compose_output_name(default_basename, \".csv\", p)\n",
    "\n",
    "    # Fallback: force .csv extension\n",
    "    p = p.with_suffix(\".csv\")\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27100ef0-5287-4279-8cfd-043cc3f0c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(text: str, basename: str, directory=Path.cwd(), ext=\".txt\"):\n",
    "    \"\"\"\n",
    "    Saves a string as text with _CoreNameYYYYMMDD suffix and README.\n",
    "    \"\"\"\n",
    "    out_path = compose_output_name(basename, ext, directory)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(out_path).write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Text saved -> {out_path}\")\n",
    "    write_readme(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'calendar years pre year of core'\n",
    "data['calendar years pre year of core'] = year_of_core - data['Age bp']\n",
    "\n",
    "from pathlib import Path\n",
    "out_dir = Path(output_file_name).parent if output_file_name else Path.cwd()\n",
    "save_dataframe(data, basename=\"AgeModel\", directory=out_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d497d",
   "metadata": {},
   "source": [
    "# Check the output data. Make sure data isnt *fishy*\n",
    "Look at the column labeled 'Age'. Are the ages within the realm of possibility? If not, ask Asmara for help!\n",
    "\n",
    "# Now plot it!\n",
    "Run cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80727060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ask for the age model file to be plotted (CSV format)\n",
    "age_model_file = input(\"Enter the full path to the age model file to plot (e.g., /path/to/age_model.csv): \")\n",
    "data = pd.read_csv(age_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3938c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the core name for the plot title\n",
    "core_name = input(\"Enter the core name for the title: \")\n",
    "\n",
    "# Ask for depths to label \"calendar years pre year of core\"\n",
    "depths_to_label_input = input(\"Enter the depths (comma-separated) where 'calendar years pre year of core' should be labeled (or type 'all' to label all intervals): \")\n",
    "\n",
    "if depths_to_label_input.lower() == 'all':\n",
    "    depths_to_label = data['Center point of interval'].tolist()  # Label all intervals\n",
    "else:\n",
    "    depths_to_label = [float(depth.strip()) for depth in depths_to_label_input.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ccf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask if any intervals have undetectable radioisotope amounts\n",
    "missing_data_input = input(\"Are there any intervals with undetectable amounts of radioisotopes? (yes/no): \").strip().lower()\n",
    "\n",
    "if missing_data_input == 'yes':\n",
    "    missing_depths_input = input(\"Enter the depths (comma-separated) with undetectable radioisotopes: \")\n",
    "    missing_depths = [float(depth.strip()) for depth in missing_depths_input.split(\",\")]\n",
    "else:\n",
    "    missing_depths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for the data series and error bars\n",
    "excess_pb210_color = 'black'\n",
    "excess_pb210_error_color = mcolors.to_rgba(excess_pb210_color, alpha=0.3)\n",
    "supported_activity_color = 'grey'\n",
    "supported_activity_error_color = mcolors.to_rgba(supported_activity_color, alpha=0.3)\n",
    "\n",
    "# Ask for the folder to save the plot PDF and create a filename\n",
    "save_location = input(\"Enter the full path where you want to save the plot PDF (e.g., /path/to/your/directory): \")\n",
    "plot_filename = f\"{core_name}_Age_Model.pdf\"\n",
    "save_path = save_location.rstrip('/') + \"/\" + plot_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b65633-9682-48ce-be5c-f00753689c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, basename: str, directory=Path.cwd(), ext=\".png\", dpi=300):\n",
    "    \"\"\"\n",
    "    Adds rotated footer 'Created by <Operator> 210PbAgeModelScript v2.2'\n",
    "    along the right axis spine in light grey, saves with suffix, and writes a README.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any prior footer\n",
    "    for t in list(fig.texts):\n",
    "        if getattr(t, \"get_gid\", lambda: None)() == \"footer\":\n",
    "            try:\n",
    "                t.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Grab the first axes (main plot)\n",
    "    ax = fig.axes[0]\n",
    "\n",
    "    # Add rotated text right on the spine\n",
    "    footer = ax.text(\n",
    "        1.01, 0.5, f\"Created by {META['operator']} with 210PbAgeModelScript {META['version']}\",\n",
    "        ha=\"left\", va=\"center\", rotation=270, fontsize=9,\n",
    "        color=\"lightgrey\", transform=ax.transAxes\n",
    "    )\n",
    "    footer.set_gid(\"footer\")\n",
    "\n",
    "    # Save figure\n",
    "    out_path = compose_output_name(basename, ext, directory)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "\n",
    "    print(f\"Figure saved -> {out_path}\")\n",
    "    write_readme(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 5))\n",
    "plt.errorbar(\n",
    "    data['Pb-210 activity (Bq/g)'], data['Center point of interval'], \n",
    "    xerr=data['Pb-210 activity Uncertainty (Bq-g)'], fmt='-', color=excess_pb210_color, \n",
    "    label='Pb-210 activity (Bq/unit)', capsize=5, linewidth=1, \n",
    "    ecolor=excess_pb210_error_color\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.xlim(0.01, 10)\n",
    "# Highlight intervals with missing radioisotopes using brown spans\n",
    "for y in missing_depths:\n",
    "    plt.axhspan(y - 0.5, y + 0.5, alpha=0.5, color='brown', \n",
    "                label='Undetectable radioisotope' if y == missing_depths[0] else None)\n",
    "\n",
    "plt.title(f\"{core_name} 210 Pb Uncorrected Activity\", fontsize=18)\n",
    "plt.xlabel(\"Bq/g\", fontsize=14)\n",
    "plt.ylabel(\"Depth (cm)\", fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Show depth from surface (invert y-axis)\n",
    "plt.grid(True, which='both', linestyle='-', linewidth=0.5, color='lightgray')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=1)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "save_figure(fig, basename=\"AgeModelPlot\", directory=save_location, ext=\".pdf\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(5, 10))\n",
    "\n",
    "# Calculate vertical errors from top/base to center of interval\n",
    "depth_errors = np.array([\n",
    "    [center - top, base - center]\n",
    "    for top, center, base in zip(\n",
    "        data['Top of interval (cm)'],\n",
    "        data['Center point of interval'],\n",
    "        data['Base of interval (cm)']\n",
    "    )\n",
    "]).T  # shape (2, N)\n",
    "\n",
    "# Calculate symmetric vertical errors\n",
    "yerr = np.abs(data['Center point of interval'] - data['Top of interval (cm)'])\n",
    "\n",
    "# Calculate symmetric horizontal errors\n",
    "xerr = data['Pb-210 activity Uncertainty (Bq-g)']\n",
    "\n",
    "# Draw connecting line for Excess Pb-210 data points\n",
    "plt.plot(\n",
    "    data['Excess Pb-210 (Bq/g)'], data['Center point of interval'],\n",
    "    color=excess_pb210_color, linewidth=1, zorder=1\n",
    ")\n",
    "\n",
    "# Draw error boxes for Excess Pb-210\n",
    "for i in range(len(data)):\n",
    "    x = data['Excess Pb-210 (Bq/g)'].iloc[i]\n",
    "    y = data['Center point of interval'].iloc[i]\n",
    "    width = xerr.iloc[i] * 2\n",
    "    height = yerr.iloc[i] * 2\n",
    "    rect = patches.Rectangle(\n",
    "        (x - xerr.iloc[i], y - yerr.iloc[i]), width, height,\n",
    "        linewidth=0.5, edgecolor='grey', facecolor='none'\n",
    "    )\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "# Plot background activity with horizontal error bars only\n",
    "plt.errorbar(\n",
    "    data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)'],\n",
    "    data['Center point of interval'],\n",
    "    xerr=data['Background activity uncertainty (Bq/g)'],\n",
    "    fmt='-', color=supported_activity_color, label='Background Activity',\n",
    "    capsize=5, linewidth=1, ecolor=supported_activity_error_color\n",
    ")\n",
    "\n",
    "# Highlight missing intervals with brown spans\n",
    "for y in missing_depths:\n",
    "    plt.axhspan(y - 0.5, y + 0.5, alpha=0.5, color='brown',\n",
    "                label='Undetectable radioisotope' if y == missing_depths[0] else None)\n",
    "\n",
    "# Annotate the selected depths with rounded-up calendar years\n",
    "for i, depth in enumerate(data['Center point of interval']):\n",
    "    if depth in depths_to_label:\n",
    "        year_value = data['calendar years pre year of core'].iloc[i]\n",
    "        if not pd.isna(year_value):\n",
    "            plt.text(\n",
    "                data['Excess Pb-210 (Bq/g)'].iloc[i] + 0.05, depth,\n",
    "                f'{int(np.ceil(year_value))}',\n",
    "                fontsize=14, color='black', verticalalignment='center'\n",
    "            )\n",
    "\n",
    "plt.title(f\"{core_name} Age Model\", fontsize=18)\n",
    "plt.xlabel(\"Bq/unit\", fontsize=14)\n",
    "plt.ylabel(\"Depth (cm)\", fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.xlim(0.01, 1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, which='both', linestyle='-', linewidth=0.5, color='lightgray')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=1)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "save_figure(fig, basename=\"AgeModelPlot\", directory=save_location, ext=\".pdf\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de65ed2-4d33-416d-8db0-1f9af0ddbddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ask user for CSV\n",
    "csv_file = input(\"Enter the CSV filename with center point age-depth data (e.g., myfile.csv): \")\n",
    "\n",
    "# Load the data\n",
    "age_model = pd.read_csv(csv_file)\n",
    "\n",
    "# Confirm columns (adjust if needed)\n",
    "depth_col = \"Center point of interval\"\n",
    "age_col = \"calendar years pre year of core\"\n",
    "\n",
    "# Sort by depth just in case\n",
    "age_model = age_model.sort_values(by=depth_col)\n",
    "\n",
    "# Get lists\n",
    "depths = age_model[depth_col].values\n",
    "ages = age_model[age_col].values\n",
    "\n",
    "# Calculate sedimentation rates between consecutive centerpoints\n",
    "print(\"\\nSedimentation rates between consecutive center points:\")\n",
    "for i in range(len(depths)-1):\n",
    "    dz = depths[i+1] - depths[i]\n",
    "    dt = abs(ages[i+1] - ages[i])\n",
    "    if dt > 0:\n",
    "        sed_rate = dz / dt  # cm/year\n",
    "        print(f\"{depths[i]}–{depths[i+1]} cm: {sed_rate:.3f} cm/year\")\n",
    "    else:\n",
    "        print(f\"{depths[i]}–{depths[i+1]} cm: undefined (no time difference)\")\n",
    "\n",
    "# Ask user if they want a custom interval\n",
    "custom = input(\"\\nWould you like to calculate a sedimentation rate over a custom depth interval? (yes/no): \")\n",
    "if custom.strip().lower() == \"yes\":\n",
    "    custom_range = input(\"Enter two depths separated by a comma (e.g., 0,11): \")\n",
    "    custom_depths = [float(d.strip()) for d in custom_range.split(\",\")]\n",
    "    if len(custom_depths) != 2:\n",
    "        print(\"⚠️ Please enter exactly two depths.\")\n",
    "    else:\n",
    "        d1, d2 = custom_depths\n",
    "        # find nearest centerpoints\n",
    "        closest_d1 = depths[np.argmin(np.abs(depths - d1))]\n",
    "        closest_d2 = depths[np.argmin(np.abs(depths - d2))]\n",
    "        age1 = ages[np.argmin(np.abs(depths - d1))]\n",
    "        age2 = ages[np.argmin(np.abs(depths - d2))]\n",
    "        dz = closest_d2 - closest_d1\n",
    "        dt = abs(age2 - age1)\n",
    "        if dt > 0:\n",
    "            sed_rate_custom = dz / dt\n",
    "            print(f\"\\nCustom interval:\")\n",
    "            print(f\"{closest_d1}–{closest_d2} cm: {sed_rate_custom:.3f} cm/year\")\n",
    "        else:\n",
    "            print(\"\\nCustom interval rate undefined (no time difference).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281d3c7",
   "metadata": {},
   "source": [
    "# Well done!\n",
    "\n",
    "#### When you've finished, go to Cell > All Output > Clear to be ready for the next user of this script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pb210)",
   "language": "python",
   "name": "pb210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
